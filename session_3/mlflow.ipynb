{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmarze/ds-notebooks/blob/main/session_3/mlflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtFz0njtLJp"
      },
      "source": [
        "\n",
        "\n",
        "## Zakres dzisiejszych zajęć:\n",
        "* wdrożenie modelu ML\n",
        "\n",
        "## Zarządzanie i wdrażanie modeli ML\n",
        "Platform ML Flow (https://www.mlflow.org/docs/latest/index.html) umożliwia całościowe zarządzanie cyklem życia modeli.\n",
        "\n",
        "Cykl życia modelu:\n",
        "\n",
        "1. Pobranie surowych danych  2. Przygotowanie danych 3. Trenowanie 4. Wdrożenie\n",
        "\n",
        "Inzynieria danych (1,2), Analityka danych (3), Inzynieria oprogramowania (4)\n",
        "\n",
        "ML FLow ma 4 podstawowe komponenty:  TRACKING, PROJECTS, MODELS, REJESTR\n",
        "* logowanie eksperymentów, wartości parametrów modeli i osiąganych przez nie wyników\n",
        "* serializowanie modeli (na potrzeby współdzielenia modelu, przeniesienia na inne środowisko lub serwowania)\n",
        "* samodzielny format do uruchamiania analiz\n",
        "* wersjonowanie modelu, adnotowanie i przechowywanie w Rejestrze\n",
        "Rozwiazanie open-source.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgE5yqHptLJr"
      },
      "source": [
        "![mlflow](https://github.com/biodatageeks/ds-notebooks/blob/main/session_3/mlflow.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yW8FvJ_tLJr"
      },
      "source": [
        "### Przygotowanie danych do analizy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5f8SMeUDtLJr"
      },
      "outputs": [],
      "source": [
        "user_name = 'mmarze' # TODO set your GitHub user name\n",
        "tracking_uri = 'https://mlflow-server-919598915406.us-central1.run.app' # TODO set your mlflow server url\n",
        "semester = '2024l' # TODO set appropriate semester\n",
        "user_id = 7067 # TODO set appropriate user id\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".config('spark.driver.memory','1g') \\\n",
        ".config('spark.executor.memory', '2g') \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://ds-2024l-9102-notebook-data/survey/2020/survey_results_public.csv /content/"
      ],
      "metadata": {
        "id": "o-i04vd_uwMc",
        "outputId": "dfb65aa3-a3dc-49fd-ab6b-84d6286396df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ds-2024l-9102-notebook-data/survey/2020/survey_results_public.csv...\n",
            "\\ [1 files][ 90.2 MiB/ 90.2 MiB]                                                \n",
            "Operation completed over 1 objects/90.2 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GtGGrRxWtLJs"
      },
      "outputs": [],
      "source": [
        "gs_path = f'/content/survey_results_public.csv'\n",
        "table_name = \"survey_2020\"\n",
        "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
        "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
        "          USING csv \\\n",
        "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
        "          LOCATION \"{gs_path}\"')\n",
        "\n",
        "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
        "                    FROM {table_name} WHERE convertedComp IS NOT NULL ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thdv9qV2tLJs"
      },
      "source": [
        "### Transformacja danych do wektora cech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DuitsOtqtLJs"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "y = 'compAboveAvg'\n",
        "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']\n",
        "\n",
        "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
        "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label')]\n",
        "\n",
        "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]\n",
        "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
        "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features')\n",
        "\n",
        "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
        "\n",
        "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
        "                          onehotencoder_stages + \\\n",
        "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
        "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234) # Podzial na zbior treningowy/testowy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDE2FctDtLJs"
      },
      "source": [
        "###  ML Flow: Definicja eksperymentu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "87rJmqkzvk30",
        "outputId": "5392204b-d900-441d-b6d0-800e45ab0625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.19.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==2.19.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.19.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.19.0->mlflow)\n",
            "  Downloading databricks_sdk-0.40.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.19.0-py3-none-any.whl (27.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.40.0-py3-none-any.whl (629 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.7/629.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 databricks-sdk-0.40.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.19.0 mlflow-skinny-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sneXTzNotLJs",
        "outputId": "3b00b33d-72d5-4663-992a-089c2f37abf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/01/19 08:47:27 INFO mlflow.tracking.fluent: Experiment with name 'classifier_mmarze' does not exist. Creating a new experiment.\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "\n",
        "mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "ename = f\"classifier_{user_name}\"\n",
        "mlflow.set_experiment(experiment_name=ename)\n",
        "experiment = mlflow.get_experiment_by_name(ename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0J3YHattLJt"
      },
      "source": [
        "### Definicja metryk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZuGiSQkRtLJt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "evaluator_auroc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_f = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKL1DZ2tLJt"
      },
      "source": [
        "### ML Flow: logowanie eksperymentu z drzewem decyzyjnym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ1RSmJutLJt"
      },
      "source": [
        "Gotowy kod potoku analizy danych instrumentalizujemy z wykorzystaniem ML Flow.\n",
        "Instrumentalizacja nie wplywa na proces trenowania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UfMBr61mtLJt",
        "outputId": "9941a689-e24d-4da0-fdae-9216508968ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run dt_model at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9/runs/c1d71eec6c1b428b8d0bc97a4c63c114\n",
            "🧪 View experiment at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
        "dt_model = Pipeline(stages=[dt]).fit(training)\n",
        "pred_dt = dt_model.transform(test)\n",
        "label_and_pred = pred_dt.select('label', 'prediction')\n",
        "\n",
        "### Instrumentalizacja kodu z uzyciem Pythonowego API ML Flow\n",
        "\n",
        "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"dt_model\"):\n",
        "\n",
        "    mlflow.set_tag(\"classifier\", \"decision_tree\")  ## ustawienie tagow\n",
        "    mlflow.log_param(\"depth\", dt.getMaxDepth())    ## zapisanie metadanych - hiperparametrow\n",
        "\n",
        "    res = dt_model.transform(test)\n",
        "\n",
        "    test_metric_auroc = evaluator_auroc.evaluate(res)\n",
        "    test_metric_acc = evaluator_acc.evaluate(res)\n",
        "    test_metric_recall = evaluator_recall.evaluate(res)\n",
        "    test_metric_prec = evaluator_prec.evaluate(res)\n",
        "    test_metric_f = evaluator_f.evaluate(res)\n",
        "\n",
        "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc)  ### zapisanie metryk\n",
        "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc)\n",
        "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall)\n",
        "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)\n",
        "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f)\n",
        "\n",
        "    # mlflow.spark.log_model(spark_model=dt_model, artifact_path='dt_classifier') ## logowanie artefaktu - serializowany model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrHNWgbFtLJt"
      },
      "source": [
        "ML Flow moze zapisac model z innej zaintegrowanej biblioteki (TF, Keras etc)\n",
        "Co widzimy w ML Flow UI?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjPdwaOhtLJt"
      },
      "source": [
        "### ML Flow: logowanie eksperymentu drzewa decyzyjnego z walidacją krzyżową"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9KdAkWrJtLJt",
        "outputId": "c8b8b493-4b01-4cc7-e946-89c339f6ee6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run best_model at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9/runs/31371c4f286d4343a589793990df18aa\n",
            "🧪 View experiment at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "param_grid = ParamGridBuilder(). \\\n",
        "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
        "    build()\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator_auroc, numFolds=4)\n",
        "\n",
        "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"best_model\"):\n",
        "    cv_model = cv.fit(training)\n",
        "\n",
        "    mlflow.log_param(\"depth\", cv_model.bestModel.depth)\n",
        "\n",
        "    res = cv_model.bestModel.transform(test)\n",
        "\n",
        "    test_metric_auroc = evaluator_auroc.evaluate(res)\n",
        "    test_metric_acc = evaluator_acc.evaluate(res)\n",
        "    test_metric_recall = evaluator_recall.evaluate(res)\n",
        "    test_metric_prec = evaluator_prec.evaluate(res)\n",
        "    test_metric_f = evaluator_f.evaluate(res)\n",
        "\n",
        "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc)\n",
        "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc)\n",
        "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall)\n",
        "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)\n",
        "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f)\n",
        "\n",
        "    # mlflow.spark.log_model(spark_model=cv_model.bestModel, artifact_path='best_classifier')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaAPfSDrtLJu"
      },
      "source": [
        "### ML Flow: logowanie eksperymentu z modelem GBT\n",
        "Wykorzystanie innego modelu, drzew decyzyjnych ze wzmocnieniem gradientowym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt7BYcHNtLJu",
        "outputId": "1359c02f-96e2-486c-99c4-2dcaab18edb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run gbt_model at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9/runs/2b57ffcbb36b4e9a9ac566ee8510a007\n",
            "🧪 View experiment at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "gbt_model = gbt.fit(training)\n",
        "\n",
        "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model\"):\n",
        "\n",
        "    mlflow.log_param(\"depth\", gbt.getMaxDepth())\n",
        "\n",
        "    res = gbt_model.transform(test)\n",
        "\n",
        "    test_metric_auroc = evaluator_auroc.evaluate(res)\n",
        "    test_metric_acc = evaluator_acc.evaluate(res)\n",
        "    test_metric_recall = evaluator_recall.evaluate(res)\n",
        "    test_metric_prec = evaluator_prec.evaluate(res)\n",
        "    test_metric_f = evaluator_f.evaluate(res)\n",
        "\n",
        "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc)\n",
        "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc)\n",
        "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall)\n",
        "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)\n",
        "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f)\n",
        "\n",
        "    # mlflow.spark.log_model(spark_model=gbt_model, artifact_path='gbt_classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pklipfh3tLJu"
      },
      "source": [
        "### ML Flow: serwowanie modelu\n",
        "\n",
        "Zeby zapewnić prosty interfejs do klasyfikatora, zapiszemy model wraz z krokami wstępnego przetwarzania (przekształcenie danych wejściowych w wektor cech)\n",
        "\n",
        "Model jest przechowany w strukturze katalogowej - wraz z konfiguracją (zaleznosciami) oraz zserializowana reprezentacją. Moze byc przechowany w kilku \"smakach\" (flavour) w naszym przypadku jest to SparkML model oraz Pythonowa funkcja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2aTvHKCttLJu",
        "outputId": "8cee54db-2b37-44cb-d03f-fc89e7cea852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run gbt_model_raw at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9/runs/b49d549e04354a4890ebe03de61c88f8\n",
            "🧪 View experiment at: https://mlflow-server-919598915406.us-central1.run.app/#/experiments/9\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "raw_training, raw_test = spark_df.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "full_classifier_name=f\"{user_name}_full_gbt_classifier\"\n",
        "version=1\n",
        "\n",
        "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model_raw\"):\n",
        "    pipeline = Pipeline(stages=stringindexer_stages + \\\n",
        "                          onehotencoder_stages + \\\n",
        "                          [vectorassembler_stage] + [gbt] )\n",
        "    model = pipeline.fit(raw_training)\n",
        "    # mlflow.spark.log_model(spark_model=model, artifact_path='gbt_classifier', registered_model_name=full_classifier_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLy7jpyFtLJu"
      },
      "source": [
        "### Klasyfikacja jako serwis REST\n",
        "\n",
        "Sekwencja komend do uruchomienia w terminalu\n",
        "\n",
        "```bash\n",
        "export USER_NAME=yourGitHubUserName # TODO change\n",
        "export MLFLOW_TRACKING_URI=https://mlflow-server-919598915406.us-central1.run.app\n",
        "export MODEL_VERSION=1\n",
        "export MODEL_NAME=full_gbt_classifier\n",
        "export MLFLOW_SERVE_PORT=9090\n",
        "cd\n",
        "/opt/conda/miniconda3/bin/python -m venv ./venv\n",
        "source ./venv/bin/activate\n",
        "mlflow models serve -m models:/${USER_NAME}_${MODEL_NAME}/${MODEL_VERSION} -p ${MLFLOW_SERVE_PORT} --no-conda\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZutbCCFktLJu"
      },
      "source": [
        "#### Przykładowe wywołanie serwisu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZYoa--ntLJu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://localhost:9090/invocations\"\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "input_data = {\n",
        "    \"dataframe_split\": {\n",
        "        \"columns\": [\"OpSys\", \"EdLevel\", \"MainBranch\", \"Country\", \"JobSeek\", \"YearsCode\"],\n",
        "        \"data\": [\n",
        "            [\n",
        "                \"MacOS\",\n",
        "                \"Master’s degree (M.A., M.S., M.Eng., MBA, etc.)\",\n",
        "                \"I am a developer by profession\",\n",
        "                \"United Kingdom\",\n",
        "                \"I am not interested in new job opportunities\",\n",
        "                \"10\"\n",
        "            ]\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.post(url, json=input_data, headers=headers)\n",
        "print(r.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsBr4lidtLJu"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoPmi2IytLJu"
      },
      "source": [
        "# Podsumowanie kursu\n",
        "\n",
        "Podczas zajęć przećwiczyliśmy następujące technologii i narzędzi z zakresu Data&AI:\n",
        "Przechowywanie:\n",
        "* rozproszony system plików (HDFS)\n",
        "* obiektowy system plików (GCS)\n",
        "\n",
        "Środowiska pracy:\n",
        "* lokalny klaster Hadoop\n",
        "* klaster Kubernetes w chmurze Google\n",
        "* notatniki Jupyter\n",
        "\n",
        "Przetwarzanie danych:\n",
        "* Apache Spark, API DataFrame i SQL\n",
        "* Pandas\n",
        "* Python\n",
        "\n",
        "Wizualizacja danych:\n",
        "* matplotlib\n",
        "* seaborn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z25IEwmVtLJv"
      },
      "source": [
        "![](https://github.com/biodatageeks/ds-notebooks/blob/main/img/ecosystem_green.png?raw=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "datascience"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "notebook_test": {
      "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
      "user": "ds-lab-testuser1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}